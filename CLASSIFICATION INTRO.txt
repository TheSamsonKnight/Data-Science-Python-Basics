CLASSIFICATION
============================================================================================================================================================================================================
-CATEGORIZING SOME UNKNOWN ITEMS INTO A DISCRETE SET OF CATEGORIES OR CLASSES
-SUPERVISED LEARNING APPROACH
-THE TARGET ATTRIBUTE IS A CATEGORICAL VARIABLE WITH DISCRETE VALUES
-ATTEMPTS TO LEARN THE RELATIONSHIP BETWEEN A SET OF FEATURE VARIABLES AND A TARGET VARIABLE OF INTEREST
-DETERMINES THE CLASS LABEL FOR AN UNLABELED TEST CASE
	GIVEN A SET OF TRAINING DATA POINTS ALONG WITH THE TARGET LABELS
-MULTI CLASS CLASSIFIER: A CLASSIFIER THAT CAN PREDICT A FIELD WITH MULTIPLE DISCRETE VALUES
-DATA CLASSIFICATION HAS SEVERAL APPLICATIONS IN A WIDE VARIETY OF INDUSTRIES
	-MANY PROBLEMS CAN BE EXPRESSED AS ASSOCIATIONS BETWEEN FEATURES AND TARGET VARIABLES, 	ESPECIALLY WHEN LABELLED DATA IS AVAIBLE > PROVIDES A BROAD RANGE OF APPLICABILITY FOR CLASSIFICATION
	-EMAIL FILTERING, SPEECH RECOGNITION, HANDWRITING RECOGNITION, BIOMETRIC ID, DOCUMENTS

-CLASSIFICATION ALGORITHMS IN ML:
	DECISION TREES(ID3, C4.5, C5,0)
	NAIVE BAYES
	LINEAR DISCRIMINANT ANALYSIS
	K NEAREST NEIGHBOR
	LOGISTIC REGRESSION
	NEURAL NETWORKS
	SUPPORT VECTOR MACHINES(SVM)
============================================================================================================================================================================================================
K NEAREST NEIGHBOURS ALGORITHM

-TAKES A BUNCH OF LABELED POINTS AND USES THEM TO LEARN HOW TO LABEL OTHER POINTS
	-CLASSIFYING CASES BASED ON THEIR SIMILARITY TO OTHER CASES
	-BASED ON SIMILAR CASES WITH THE SAME CLASS LABELS ARE NEAR EACH OTHER
		-THE DISTANCE BETWEEN TWO CASES IS A MEASURE OF THEIR DISSIMILARITY	
	-CASES/DATA NEAR EACH OTHER ARE SAID TO BE "NEIGHBORS"
-GIVEN DATASET WITH PREDEFINED LABELS, NEED TO BUILD A MODEL TO BE USED TO PREDICT THE CLASS OF A NEW OR UNKNOWN CASE
-CAN ALSO BE USED FOR REGRESSION

-EUCLIDEAN DISTANCE:

ALGORITHM:
	-PICK VALUE FOR K
	-CALCULATE THE DISTANCE OF UNKNOWN CASES FROM ALL CASES
	-SELECT THE K-OBSERVATIONS IN THE TRAINING DATA THAT ARE "NEAREST" TO THE UNKNOWN DATA POINT
	-PREDICT THE RESPONSE OF THE UNKNOWN DATA POINT USING THE MOST POPULAR RESPONSE VALUE FROM THE 	K-NEAREST NEIGHBORS
============================================================================================================================================================================================================
EVALUATION METRICS IN CLASSIFICATION

-DESCRIBE THE PERFORMANCE OF A MODEL
	KEY ROLE IN MODEL DEV: PROVIDE INSIGHT

-JACCARD INDEX: Y-ACTUAL LABERLS	^Y-PREDICTED LABELS
	HIGHER VALUE = HIGHER ACCURACY

-F1 SCORE: HARMONIC AVERAGE OF THE PRECISION AND RECALL(USES CONFUSION MATRIX)
	=2X(PRECISION X RECALL)/(PRECISION + RECALL)
	-HIGHER ACCURACY CLOSE TO 1
		-PRECISION: MEASURE OF ACCURACY, PROVIDED THAT A CLASS LABEL HAS BEEN PREDICTED
			=TRUE POSITIVE/(TRUE POSITIVE + FALSE POSITIVE)
		-RECALL: TRUE POSITIVE RATE
			=TRUE POSITIVE/(TRUE POSITIVE + FALSE NEGATIVE)
	-JACCARD AND F1 SCORE CAN BE USED FOR MULTI CLASS CLASSIFIERS

-LOG LOSS:MEASURES PERFORMANCE OF A CLASSIFIER WHERE THE PREDICTED OUTPUT IS A PROBABILITY VALUE 0TO1
	-HIGHER ACCURACY WHEN CLOSER TO 0
============================================================================================================================================================================================================
DECISION TREES

-BASIC INTUITION BEHIND A DECISION TREE IS TO MAP OUT ALL POSSIBLE DECISION PATHS IN THE FORM OF A TREE
-TESTING AN ATTRIBUTE AND BRANCHING THE CASES BASED ON THE RESULTS OF THE TEST
	-EACH INTERNAL NODE CORRESPONDS TO A TEST
	-EACH BRANCH RELATES TO A RESULT OF THE TEST
	-EACH LEAD NODE ASSIGNS A VALUE TO A CLASS
-USE TRAINING PART OF THE DATA SET TO BUILD A DECISION TREE(SPLITTING THE TRAINING SET INTO DISTINCT NODES)
	USE DT TO PREDICT THE CLASS OF UNKNOWN VALUE
	ONE NODE IN A DECISION TREE CONTAINS ALL/MOST OF ONE CATEGORY OF THE DATA
-CONSTRUCTED BY CONSIDERING THE ATTRIBUTES ONE BY ONE
	-CHOOSE AN ATTRIBUTE FROM DATASET
	-CALCULATE THE SIGNIFICANCE OF ATTRIBUTE IN SPLITTING OF DATA
	-SPLIT DATA BASED ON THE VALUE OF THE BEST ATTRIBUTE
	-REPEAT FOR THE REST OF THE ATTRIBUTES
	-USE DT TO PREDICT THE CLASS OF UNKNOWN CASES
============================================================================================================================================================================================================
BUILDING DECISION TREES

-BUILT USING RECURSIVE PARTITIONING TO CLASSIFY THE DATA
-WHICH ATTRIBUTE IS THE BEST/PREDICTIVE TO SPLIT DATA BASED ON THE FEATURE
	-PREDICTIVENESS IS BASED ON DECREASE IN IMPURITY OF NODES
	-LOOK FOR BEST FEATURE TO DECREASE THE IMPURITY OF THE FEATURE IN THE LEAVES AFTER SPLITTING 	THEM UP BASED ON THAT FEATURE
	-THE CHOICE OF ATTRIBUTE TO SPLIT DATA IS VERY IMPORTANT, ALL ABOUT PURITY OF THE LEAVES AFTER THE SPLIT 
	-A NODE IN THE TREE IS CONSIDERED PURE IF IN 100 PERCENT OF THE CASES THE NODES FALL INTO A 	SPECIFIC CATERGORY OF THE TARGET FIELD
	-THE METHOD USES RECURSIVE PARTITIONING TO SPLIT THE TRAINING RECORDS INTO SEGMENTS BY 	MINIMIZING THE IMPURITY OF EACH STEP

-IMPURITY OF NODES IS CALCULATED BY ENTROPY OF DATA IN THE NODE
	-ENTROPY: THE AMOUNT OF INFORMATION DISORDER/MEASURE OF RANDOMNESS OR UNCERTAINTY 
	-THE LOWER THE ENTROPY THE LESS UNIFORM THE DISTRIBUTION, THE PURER THE NODE
	-THE ENTROPY OF THE NODE DEPENDS ON HOW MUCH RANDOM DATA IS IN THAT NODE AND IS CALCULATED FOR EACH NODE

-IN DT, LOOK FOR TREES THAT HAVE THE SMALLEST ENTROPY IN THEIR NODES
-ENTROPY IS USED TO CALCULATE THE HOMOGENEITY OF THE SAMPLES IN THAT NODE
	-COMPLETELY HOMOGENEOUS: ENTROPY IS 0
	-EQUALLY DIVIDED: ENTROPY IS 1

ENTROPY = P(A)LOG(P(A))-P(B)LOG(P(B))	P: PORTION/RATIO OF CATEGORY
-WHHICH ATTRIBUTE IS BEST?
	THE TREE WITH THE HIGHER INFORMATION GAIN AFTER SPLITTING
-INFORMATION GAIN: THE INFO THAT CAN INCREASE THE LEVEL OF CERTAINTY AFTER SPLITTING
	INFORMATION GAIN = (ENTROPY BEFORE SPLIT)-(WEIGHTED ENTROPY AFTER SPLIT)
-INFORMATION GAIN AND ENTROPY CAN THOUGHT OF AS OPPOSITES
	-AS ENTROPY-THE AMOUNT OF RANDOMNESS DECREASES, THE INFORMATION GAIN-THE AMOUNT OF CERTAINTY INCREASES (AND VICE VERSA)
============================================================================================================================================================================================================
LOGISTICAL REGRESSION

-A STATISTICAL AND ML TECHNIQUE FOR CLASSIFYING RECORDS OF A DATASET BASED ON VALUES OF INPUT FIELDS
	-CLASSIFICATION ALGORITHM FOR CATEGORICAL VARIABLES
	-LOGISTIC IS ANALOGOUS TO LINEAR BUT PREDICTS CATEGORICAL/DISCRETE TARGET FIELD INSTEAD OF #S
	-BINARY VALUES YES/NO, TRUE/FALSE, POSITIVE/NEGATIVE, SUCCESSFUL/NOT SUCCESSFUL, 0/1
-INDEPENDENT VARIABLES SHOULD BE CONTINUOUS
	-CAN BE USED FOR BINARY CLASSIFICATION AND MULTI-CLASS CLASSIFICATION
-PREDICT THE CLASS OF EACH CASE, ALSO MEASURE PROBABILITY OF A CASE BELONGING TO SPECIFIC CLASS
-EXPLAINS INFLUENCE OF IND VAR HAS ON DEP VAR WHILE CONTROLLING OTHER IND VAR

LINEAR REGRESSION
-TRIES TO PREDICT A CONTINOUS VALUE OF VARIABLES
-CANNOT PROPERLY MEASURE THE PROBABILITY OF A CASE BELONGING TO A CLASS

LOGISTICAL REGRESSION TRAINING
-MAIN GOAL OF LOGISTIC REGRESSION IS TO CHANGE THE PARAMETERS OF THE MODEL, SO AS TO BE THE BEST ESTIMATION OF THE LABELS OF THE SAMPLES IN THE DATASET
-GENERAL COST FUNCTION: THE DIFFERENCE BETWEEN THE ACTUAL VALUES OF Y AND MODEL OUTPUT Y-HAT
-HOW TO FIND THE BEST PARAMETERS FOR MODEL: MINIMIZE THE COST FUNCTION
-HOW TO MINIMIZE THE COST FUNCTION: USING GRADIENT DESCENT
-WHAT IS GRADIENT DESCENT: A TECHNIQUE TO USE THE DERIVATIVE OF A COST FUNCTION TO CHANGE THE PARAMETER VALUES, IN ORDER TO MINIMIZE THE COST

============================================================================================================================================================================================================
SUPPORT VECTOR MACHINES

-A SUPERVISED ALGORITHM THAT CAN CLASSIFY CASES BY FINDING A SEPARATOR
	-MAPPING DATA TO A HIGH DIMENSIONAL FEATURE SPACE, SO THAT DATA POINTS CAN BE CATEGORIZED
	-A SEPARATOR IS ESTIMATED FOR THE DATA; DATA TRANSFORMED IN SUCH A WAY THAT A SEPARATOR CAN BE DRAWN AS A HYPERPLANE (KERNELLING)
	-CHOOSE HYPERPLANE WITH BIG MARGIN AS POSSIBLE
-SUPPORT VECTORS:DATA POINTS CLOSEST TO HYPERPLANE

-CLASSIFIER TO TRAIN MODEL TO UNDERSTAND PATTERNS WITHIN THE DATA THAT MIGHT SHOW BENIGN/MALIGNANT CELLS  >>> PREDICT NEW OR UNKNOWN CELLS WITH HIGH ACCURACY

PRO:
-ACCURATE IN HIGH DIMENSIONAL SPACES
-MEMORY EFFICIENT: USES DATA ALREADY IN DATASET

CON:
-ALGORITHM IS PRONE FOR OVER FITTING
-NO PROBABILITY ESTIMATION
-SMALL DATASETS, NOT GOOD FOR MORE THAN 1000 ROWS

APPLICATIONS: 
IMAGE RECOGNITION(PICTURE CLASSIFICATION, HAND WRITTEN DIGIT)
TEXT MINING TASKS
DETECTING SPAM
SENTIMENT ANALYSIS
GENE EXPRESSION DATA CLASSIFICATION
REGRESSION, OUTLIER DECTECTION, CLUSTERING