CLUSTERING
============================================================================================================================================================================================================
A GROUP OF OBJECTS THAT ARE SIMILAR TO OTHER OBJECTS IN THE CLUSTER, AND DISSIMILAR TO DATA POINTS IN OTHER CLUSTERS

-FINDING CLUSTERS IN A DATASET >>> CAN GROUP DATA ONLY UNSUPERVISED
	-BASED ON THE SIMILARITY OF CUSTOMERS TO EACH OTHER
	-PARTION CUSTOMERS INTO MUTUALLY EXCLUSIVE GROUPS

-CUSTOMER SEGMENTATION: THE PRACTICE OF PARTITIONING A CUSTOMER BASE INTO GROUPS OF INDIVIDUALS THAT HAVE SIMILAR CHARACTERISTICS
	-STRATEGY THAT ALLOWS BUSINESS TO TARGET SPECIFIC GROUPS OF CUSTOMERS TO EFFECTIVELY ALLOCATE 	MARKETING RESOURCES
	-GENERAL SEGMENTATION PROCESS ISNT FEASIBLE FOR LARGE VOLUMES OF VARIED DATA
	-NEED ANALYTIC APPROACH TO DERIVING SEGMENTS AND GROUPS FROM LARGE DATASETS
	-HOW CUSTOMERS ARE SIMILAR TO EACH OTHER
-CLASSIFICATION IS SUPERVISED LEARNING WHERE EACH TRAINING FATA INSTANCE BELONGS TO A PARTICULAR CLASS
-CLUSTERING IS DATA IS UNLABELED AND THE THE PROCESS IS UNSUPERVISED

RETAIL/MARKETING:
	-ID BUYING PATTERNS OF CUSTOMERS
	-RECOMMEND NEW PRODUCTS/SERVICES
BANKING:
	-FRAUD DETECTION IN CC USE
	-ID CLUSTERS OF CUSTOMERS > LOYALTY POINTS
INSURANCE:
	-FRAUD DETECTION IN CLAIMS
	-INSURANCE RISK OF CUSTOMERS
PUBLICATION:
	-AUTO CATEGORIZING NEWS BASED ON THEIR CONTENT
	-RECOMMENDING SIMILAR NEWS ARTICLES
MEDICINE:
	-CHARACTERIZING PATIENT BEHAVIOR
BIOLOGY:
	-CLUSTERING GENETIC MARKERS TO ID FAMILY TIES OR HEREDITARY ILLNESS

WHY CLUSTERING:
-EXPLORATORY DATA ANALYSIS
-SUMMARY GENERATION
-OUTLIER DETECTION
-FINDING DUPLICATES
-PREPROCESSING STEP

CLUSTERING ALGORITHMS
PARTION BASED: PRODUCE SPHERE LIKE CLUSTERS
	-RELATIVELY EFFICIENT
	-MEDIUM/LARGE DATABASES
	-K MEANS, K MEDIAN, FUZZY C MEANS
HIERARCHICAL: PRODUCE TREES OF CLUSTERS
	-VERY INTUITIVE
	-SMALL DATASETS
	-AGGLOMERATIVE, DIVISIVE
DENSITY BASED: PRODUCE ARBITRARY SHAPED CLUSTERS
	-GOOD FOR SPACIAL CLUSTERS
	-"NOISE" IN DATASET
	-DBSCAN
============================================================================================================================================================================================================
K-MEANS

-PARTITIONING CLUSTERING, K = # OF CLUSTERS
-DIVIDES THE DATA INTO NON-OVERLAPPING SUBSETS(CLUSTERS) WITHOUT ANY CLUSTER INTERNAL STRUCTURE
-UNSUPERVISED ALGORITHM
	SAMPLES IN A CLUSTER ARE VERY SIMILAR
	SAMPLES ACROSS DIFFERENT CLUSTERS ARE VERY DIFFERENT
-CONVENTIONALLY, THE DISTANCE OF SAMPLES FROM EACH OTHER IS USED TO SHAPE THE CLUSTERS
	INTRA-CLUSTER DISTANCES ARE MINIMIZED(DISTANCE WITH IN CLUSTER)
	INTER-CLUSTER DISTANCES ARE MAXIMIZED(DISTANCE BETWEEN CLUSTER)

ALGORITHM:
-RANDOMLY PLACE K CENTRIODS
-CALCULATE THE DISTANCE OF EACH POINT FROM EACH CENTRIOD
-ASSIGN EACH DATA POINT(OBJECT) TO ITS CLOSEST CENTRIOD, CREATING A CLUSTER
-RECALCULATE THE POSITION OF THE K CENTRIODS
-ITERATIVE; REPEAT UNTIL THE CENTRIODS NO LONGER MOVE

ACCURACY:
-EXTERNAL APPROACH: COMPARE THE CLUSTERS WITH THE GROUND TRUTH
-INTERNAL APPROACH: AVG THE DISTANCE BETWEEN DATA POINTS WITHIN A CLUSTER

CHOOSING K:
-AMBIGUOUS, DEPENDS ON THE SHAPE AND SCALE OF THE DISTRIBUTION OF POINTS IN DATASET
-METRIC OF ERROR: AVG OF THE DISTANCE OF DATA POINTS FROM THEIR CLUSTER CENTRIODS
	-IMPLIES DENSITY/MINIMIZE THE ERROR OF CLUSTERING
-INCREASING THE NUMBER OF CLUSTERS, THE DISTANCE OF CENTRIODS TO DATAPOINTS WILL ALWAYS REDUCE
	-MORE K WILL ALWAYS DECREASE THE ERROR
-ELBOW METHOD:THE CORRECT K VALUE IS WHERE THE RATE OF DECREASE SHARPLY SHIFTS (ELBOW POINT)
============================================================================================================================================================================================================
HIERARCHICAL CLUSTERING

-ALGORITHM BUILDS A HIERARCHY OF CLUSTERS WHERE EACH NODE IS A CLUSTER CONSISTS OF THE CLUSTERS OF ITS DAUGHTER NODES

-DIVISIVE: TOP-DOWN
-AGGLOMERATIVE: DOWN-TOP

AGGLOMERATIVE CLUSTERING: METHOD BUILDS THE HIERARCHY FROM INDIVIDUAL ELEMENTS BY PROGRESSIVELY MERGING CLUSTERS
	-DOESNT REQUIRE A PRESPECIFIED NUMBER OF CLUSTERS

CREATE N CLUSTERS, ONE FOR EACH DATA POINT
COMPUTE THE PROXIMITY MATRIX
REPEAT:	MERGE THE TWO CLOSEST CLUSTERS
	UPDATE THE PROXIMITY MATRIX
UNTIL ONLY A SINGLE CLUSTER REMAINS

DISTANCE BETWEEN CLUSTERS:
	SINGLE-LINKAGE:MINIMUM DISTANCE BETWEEN CLUSTERS
	COMPLETE-LINKAGE:MAXIMUM DISTANCE BETWEEN CLUSTERS
	AVERAGE LINKAGE:AVG DISTANCE BETWEEN CLUSTERS
	CENTRIOD LINKAGE:DISTANCE BETWEEN CLUSTER CENTRIODS

PRO:
-DOESNT REQUIRE NUMBER OF CLUSTERS TO BE SPECIFIED
-EASY TO IMPLEMENT
-PRODUCES A DENDROGRAM, WHICH HELPS WITH UNDERSTANDING DATA

CON:
-CAN NEVER UNDO ANY PREVIOUS STEPS THROUGHOUT THE ALGORITHM
-GENERALLY HAS LONG RUNTIMES
-SOMETIMES DIFFICULT TO ID THE NUMBER OF CLUSTERS BY THE DENDROGRAM

KMEANS:
-MUCH MORE EFFICIENT
-REQUIRES K TO BE SPECIFIED
-GIVES ONLY ONE PARTIONING OF THE DATA BASED ON THE PREDEFINED NUMBER OF CLUSTERS
-POTENTIALLY RETURNS DIFF CLUSTERS EACH TIME IT IS RUN DUE TO RANDOM INITIALIZATION OF CENTRIODS

HIERARCHICAL:
-CAN BE SLOW FOR LARGE DATASETS
-DOESNT REQUIRE THE NUMBER OF CLUSTERS TO RUN
-GIVES MORE THAN ONE PARTITIONING DEPENDING ON THE RESOLUTINO
-ALWAYS GENERATES THE SAME CLUSTERS
============================================================================================================================================================================================================
DBSCAN

-DENSITY BASED CLUSTERING ALGORITHM: APPROPRIATE TO USE WHEN EXAMINING SPATIAL DATA
	-THE NUMBER OF POINTS WITHIN A SPECIFIED RADIUS
	-ELEMENTS IN SAME CLUSTER DONT SHARE ENOUGH SIMILARITY
	-PERFORMANCE MAY BE POOR
-GOOD FOR CLASS ID ON A SPATIAL CONTEXT

-KMEANS: ASSIGNS ALL POINTS TO A CLUSTER EVEN IF THEY DO NOT BELONG IN ANY
-DB: LOCATES REGIONS OF HIGH DENSITY AND SEPARATES OUTLIERS

DBSCAN
-VERY COMMON CLUSTERING ALGORITHM
-BASED ON DENSITY OF OBJECTS
R(RADIUS OF NEIGHBORHOOD)
-INCLUDES ENOUGH NUMBER OF POINTS WITHIN(DENSE AREA)
M(MIN NUMBER OF NEIGHBORS)
-MINIMUM NUMBER OF DATA POINTS WE WANT IN A NEIGHBORHOOD TO DEFINE A CLUSTER

-CORE POINT:POINT WITHIN NEIGHBORHOOD OF THE POINT WITH ATLEAST M POINTS
-BORDER POINT: POINT CONTAINS LESS THAN M POINTS;REACHABLE FROM SOME CORE POINT
-OUTLIER POINT:NOT A CORE POINT; NOT CLOSE ENOUGH TO BE REACHABLE FROM CORE POINT

-CLUSTER IS FORM WHENN AT LEAST ONE CORE POINT PLUS ALL REACHABLE CORE POINTS PLUS ALL THEIR BORDERS
	-ARBITRARY SHAPED CLUSTERS
	-ROBUST TO OUTLIERS
	-DOESNT REQUIRE SPECIFIC NUMBER OF CLUSTERS