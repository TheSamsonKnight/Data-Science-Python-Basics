MODEL DEVELOPMENT
============================================================================================================================================================================================================
-A MODEL CAN BE THOUGHT OF AS A MATHEMATICAL EQUATION USED TO PREDICT A VALUE GIVEN ONE OR MORE OTHER VALUES
-RELATING ONE OR MORE INDEPENDENT VARIABLES TO DEPENDENT VARIABLES
-THE MORE RELEVANT DATA YOU HAVE THE MORE ACCURATE YOUR MODEL IS

Define the explanatory variable and the response variable: Define the response variable (y) as the focus of the experiment and the explanatory variable (x) as a variable used to explain the change of the response variable. Understand the differences between Simple Linear Regression because it concerns the study of only one explanatory variable and Multiple Linear Regression because it concerns the study of two or more explanatory variables.

Evaluate the model using Visualization: By visually representing the errors of a variable using scatterplots and interpreting the results of the model.

Identify alternative regression approaches: Use a Polynomial Regression when the Linear regression does not capture the curvilinear relationship between variables and how to pick the optimal order to use in a model.

Interpret the R-square and the Mean Square Error: Interpret R-square (x 100) as the percentage of the variation in the response variable y  that is explained by the variation in explanatory variable(s) x. The Mean Squared Error tells you how close a regression line is to a set of points. It does this by taking the average distances from the actual points to the predicted points and squaring them.
============================================================================================================================================================================================================
LINEAR REGRESSION AND MULTI LINEAR REGRESSION
-LINEAR REGRESSION WILL REFER TO ONE INDEPENDENT VARIABLES TO MAKE A PREDICTION
-MULITPLE LINEAR REGRESSION WILL REFER TO MULTI INDEPENDEDNT VARIABLES TO MAKE A PREDICTION

-SIMPLE LINEAR REGRESSION: A METHOD FOR UNDERSTAND THE RELATIONSHIP BETWEEN TWO VARIABLES
	-THE PREDICTOR, INDEPENDENT VARIABLE X
	-THE TARGET, DEPENDENT VARIABLE Y
FROM SKLEARN.LINEAR_MODEL IMPORT LINEARREGRESSION
LM=LINEARREGRESSION()
X= DF[["HIGHWAY-MPG"]]
Y= DF['PRICE']
LM.FIT(X,Y)
YHAT=LM.PREDICT(X)

-MULTIPLE LINEAR REGRESSION: A METHOD USED TO EXPLAIN THE RELATIONSHIP BETWEEN
	-ONE CONTINUOUS TARGET Y VARIABLE
	-TWO OR MORE PREDICTOR X VARIBLES
Z = DF[['HORSEPOWER', 'CURB-WEIGHT', 'ENGINE SIZE', 'MPG']]
LM.FIT(Z, DF['PRICE'])
YHAT=LM.PREDICT(X)
============================================================================================================================================================================================================
MODEL EVALUATION USING VIZUALIZATION

REGRESSION PLOT: GIVES A GOOD ESTIMATION:
	-THE RELATIONSHIP BETWEEN TWO VARIABLES
	-THE STRENGTH OF THE CORRELATION
	-THE DIRECTION OF THE RELATIONSHIP(POSITIVE OR NEGATIVE) 
-HORIZONTAL AXIS IS THE INDEPENDENT VARIABLE X, VERTICAL AXIS IS THE DEPENDENT VARIABLE Y

IMPORT SEABORN AS SNS
SNS.REGPLOT(X="FEATURE-NAME", Y="TARGET", DATAFRAME-NAME=DF)
PLT.YLIM(0,)

RESIDUAL PLOT: REPRESENTS THE ERROR BETWEEN THE ACTUAL VALUE
	-RANDOMLY SPACING POINTS ARE GOOD
	-NOT RANDOMLY SPREAD OUT AROUND THE X AXIS IS BAD
IMPORT SEADBORN AS SNS
SNS.RESIDPLOT(DF['FEATURE-NAME'], DF['TARGET-NAME'])

DISTRIBUTION PLOT: COUNTS THE PREDICTED VALUE VERSUS THE ACTUAL VALUE
	-GREAT FOR VISUALIZATION FOR MODELS WITH MORE THAN ONE INDEPENDENT VARIABLE OR FEATURE
IMPORT SEABORN AS SNS
AXL = SNS.DISTPLOT(DF['PRICE'], HIST=FALSE, COLOR="R", LABEL="ACTUAL VALUE")
SNS.DISTPLOT(YHAT, HIST=FALSE, COLOR="B", LABEL="FITTED VALUES", AX=AXL)
============================================================================================================================================================================================================
POLYNOMIAL REGRESSION AND PIPELINES

-PIPELINES ARE A WAY TO SIMPLIFY CODE
-POLYNOMIAL REGRESSIONS: A SPECIAL CASE OF THE GENERAL LINEAR REGRESSION
-USEFUL FOR DESCRIBING A CURVILINEAR RELATIONSHIPS(BY SQUARING OR SETTING HIGHER-ORDER TERMS OF THE PREDICTOR VARIABLES), TRANSFORMS THE DATA
-MODEL CAN BE QUADRATIC(PREDICTOR VARIABLE IN THE MODEL IS SQUARED)
-MODEL CAN BE CUBIC(PREDICTOR VARIABLE IS CUBED)
-THE RELATIONSHIP BETWEEN THE VARIABLE AND THE PARAMETER IS ALWAYS LINEAR

F=NP.POLYFIT(X,Y,3)	***3RD ORDER
P=NP.POLYLD(F)
PRINT(P)

-POLYNOMIAL REGRESSION WITH MORE THAN ONE DIMENSION
FROM SKLEARN.PREPROCESSING IMPORT POLYNOMIALFEATURES
PR=POLYNOMIALFEATURES(DEGREE=2, INCLUDE_BIAS=FALSE)
X_POLLY=PR.FIT_TRANSFORM(X[['HORSEPOWER', 'WEIGHT']])

PRE=PROCESSING
FROM SKLEARN.PREPROCESSING IMPORT STANDARDSCALER
SCALE=STANDARDSCALER()
SCALE.FIT(X_DATA[['HORSEPOWER', 'MPG']])
X_SCALE = SCALE.TRANSFORM(X_DATA[['HORSEPOWER', 'MPG']])

-PIPELINE: NORMALIZATION > POLYNOMIAL TRANSFORM > LINEAR REGRESSION
		TRANSFORMATIONS				PREDICTION
FROM SKLEARN.PREPROCESSING IMPORT POLYNOMIALFEATURES
FROM SKLEARN.LINEAR_MODEL IMPORT LINEARREGRESSION
FROM SKLEARN.PREPROCESSING IMPORT STANDARDSCALER
FROM SKLEARN.PIPELINE IMPORT PIPELINE
INPUT=[('SCALE', STANDARDSCALER()),('POLYNOMIAL', POLYNOMIALFEATURES(DEGREE=2), ('MODE',LINEARREGRESSION()]
PIPE.FIT(DF[['HORSEPOWER', 'WEIGHT', 'ENGINE', 'MPG']],Y)
YHAT = PIPE.PREDICT(X[['HORSEPOWER', 'WEIGT', ENGINE', 'MPG']])
============================================================================================================================================================================================================
MEASURES FOR IN-SAMPLE EVALUATION

-A WAY TO NUMERICALLY DETERMINE HOW GOOD THE MODEL FITS ON DATASET
-TWO IMPORTANT MEASURES TO DETERMINE THE FIT OF A MODEL:
	-MEAN SQUARED ERROR
		FROM SKLEARN.METRICS IMPORT MEAN_SQUARED_ERROR
		MEAN_SQUARED_ERROR(DF['PRICE'], Y_PREDICT_SIMPLE_FIT)
	-R-SQUARED/COEFFICIENT OF DETERMINATION
		-A MEASURE TO DETERMINE HOW CLOSE THE DATA IS TO THE FITTED REGRESSION LINE
		-THE PERCENTAGE OF VARIATION OF THE TARGET VARIABLE (Y) THAT IS EXPLAINED BY THE LINEAR MODEL
		-COMPARING A REGRESSION MODEL TO A SIMPLE MODEL/THE MEAN OF THE DATA POINTS
		-IF THE VARIABLE X IS A GOOD PREDICTOR, THE MODEL SHOULD PERFORM MUCH BETTER THAN JUST THE MEAN
		X=DF[['MPG']]
		Y=DF[['PRICE']]
		LM.FIT(X,Y)
		LM.SCORE(X,Y)
		-VALUE IS USUALLY BETWEEN 0 AND 1
============================================================================================================================================================================================================
PREDICTION AND DECISION MAKING

-DO PREDICTED VALUES MAKE SENSE
	LM.FIT(DF['MPG'],DF['PRICES']
	LM.PREDICT(NP.ARRAY(30.0).RESHAPE(-1,1))
	LM.COEF_

	IMPORT NUMPY AS NP
	NEW_INPUT=NP.ARANGE(1,101,1).RESHAPE(-1,1)

	YHAT=LM.PREDICT(NEW_INPUT)
-VISUALIZATION
	REGRESSION
	RESIDUAL PLOT
	DISTRIBUTION PLOT
	MEAN SQUARE ERROR
	R-SQUARED
-NUMERICAL MEASURES FOR EVALUATION

-COMPARING MLR AND SLR
	-MEAN SQUARE ERROR FOR A MULTIPLE LINEAR REGRESSION MODEL WILL BE SMALLER THAN THE MEAN SQUARE ERROR FOR A SIMPLE LINEAR REGRESSION MODEL, SINCE THE ERRORS OF THE DATA WILL DECREASE WHEN MORE VARIABLES ARE INCLUDED IN THE MODEL
	-POLYNOMIAL REGRESSION WILL ALSO HAVE A SMALLER MEAN SQUARE ERROR THAN THE LINEAR REGUALR REGRESSION