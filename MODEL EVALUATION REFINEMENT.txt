MODEL EVALUATION AND REFINEMENT
============================================================================================================================================================================================================
Identify over-fitting and under-fitting in a predictive model: Overfitting occurs when a function is too closely fit to the training data points and captures the noise of the data. Underfitting refers to a model that can't model the training data or capture the trend of the data.

Apply Ridge Regression to linear regression models: Ridge regression is a regression that is employed in a Multiple regression model when Multicollinearity occurs.

Tune hyper-parameters of an estimator using Grid search: Grid search is a time-efficient tuning technique that exhaustively computes the optimum values of hyperparameters performed on specific parameter values of estimators.

-TELLS US HOW THE MODEL WORKS IN A REALTIME ENVIRONMENT 
-IN-SAMPLE EVALUATION TELLS US HOW WELL OUR MODEL WILL FIT THE DATA USED TO TRAIN IT
	DOESNT GIVE AN ESTIMATE HOW WELL THE TRAINED MODEL CAN BE USED TO PREDICT NEW DATA
	SOLUTIONS: USE IN SAMPLE DATA OR TRAINING DATA TO TRAIN THE MODEL
		OUT-OF-SAMPLE EVALUATION/TEST SET: USED TO APPROXIMATE HOW THE MODEL PERFORMS IN REAL

TRAINING/TESTING SET: SEPERATING DATA INTO TRAINING AND TESTING SETS IN AN IMPORTANT PART
	TRAINING SET: 70% OF DATA, BUILD AND TRAIN THE MODEL
	TESTING SET: 30% OF DATA, ASSESS THE PERFORMANCE OF THE PREDICTIVE MODEL
-SPIT DATA INTO RANDOM TRAIN AND TEST SUBSETS
	FUNCTION TRAIN_TEST_SPLIT()
	FROM SKLEARN.MODEL_SELECTION IMPORT TRAIN_TEST_SPLIT
	X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = TRAIN_TEST_SPLIT(X_DATA, Y_DATA, 		TEST_SIZE=0.3,RANDOM_STATE=0)
		X DATA: FEATURES/INDEPENDENT VAIRABLES
		Y DATA: DATASET TARGET:DF['PRICE']
		X_TRAIN, Y_TRAIN: PARTS OF AVAILABLE DATA AS TRAINING SET
		x_TEST, Y_TEST: PARTS OF AVAILABLE DATA AS TESTING SET
		TEST_SIZE: PERCENTAGE OF THE DATA FOR TESTING SET
		RANDOM_STATE: NUMBER GENERATOR USEDMFPR RANDOM SAMPLING

GENERALIZATION PERFORMANCE
-GENERALIZATION ERROR IS MEASURE OF HOW WELL DATA DOES AT PREDICTING PREVISOULY UNSEEN DATA
-THE ERROR WE OBTAIN USING OUR TESTING DATA IS AN APPROMIMATION OF THE ERROR
-CROSS VALIDATION: MOST COMMON OUT-OF-SAMPLE EVALUATION METRICS
	MORE EFFECTIVE USE OF DATA(EACH OBSERVATION IS USED FOR BOTH TRAINING AND TESTING)
	-DATA SET IS SPLIT INTO X EQUAL GROUPS(EACH GROUP IS REFFERED TO AS A FOLD)
	-FOLDS ARE USED TO TRAIN THE MODEL, THE REST ARE USED AS TEST SET
	-REPEATED UNTIL ALL FOLDS ARE USED BOTTH FOR TRAINING AND TESTING
	-IN THE END, WE USED THE AVG RESULTS AS THE ESTIMATE OF OUT-OF-SAMPLE ERROR
FROM SKLEARN.MODEL_SELECTION IMPORT CROSS_VAL_SCORE
SCORES = CROSS_VAL_SCORE(LR, X_DATA, Y_DATA, CV=3)
NP.MEAN(SCORES)
	
-FUNCTION CROSS_VAL_PREDICT():RETURNS THE PREDICTION THAT WAS OBTAINED FOR EACH ELEMENT WHEN IT WAS IN THE TEST SET
	HAS A SIMILAR INTERFACE TO CROSS_VAL
FROM SKLEARN.MODEL_SELECTION IMPORT CROSS_VAL_PREDICT
YHAT = CROSS_VAL_PREDICT(LR2E, X_DATA, Y_DATA, CV=3)
============================================================================================================================================================================================================
MODEL SELECTION
-UNDERFITTING: WHEN THE MODEL IS TOO SIMPLE TO FIT THE DATA
-OVERFITTING: WHEN THE MODEL IS TOO FLEXIBLE AND FITS THE NOISE RATHER THAN THE FUNCTION

RSQU_TEST=[]
ORDER = [1,2,3,4]
FOR N IN ORDER:
	PR=POLYNOMIALFEATURES(DEGREE=N)
	X_TRAIN_PR=PR.FIT_TRANSFORM(X_TRAIN[['HORSEPOWER']])
	X_TEST_PR=PR.FIT_TRANSFORM(X_TEST[['HORSEPOWER']])
	LR.FIT(X_TRAIN_PR,Y_TRAIN)
	RSQU_TEST.APPEND(LR.SCORE(X_TEST_PR,Y_TEST))
============================================================================================================================================================================================================
RIDGE REGRESSION INTRODUCTION
Ridge regression is a regression that is employed in a Multiple regression model when Multicollinearity occurs. Multicollinearity is when there is a strong relationship among the independent variables. Ridge regression is very common with polynomial regression.  The next video shows how Ridge regression is used to regularize and reduce the standard errors to avoid over-fitting a regression model
-PREVENTS OVERFITTING
	-PROBLEM WHEN HAVING MULTIPLE INDEPENDENT VARIABLES/FEATURES
-CONTROLS THE MAGNITUDE OF THESE POLYNOMIAL COEFFICIENTS BY INTRODUCING THE PARAMETER ALPHA
	-ALPHA: A PARAMETER WE SELECT BEFORE FITTING OR TRAINING THE MODEL
FROM SKLEARN.LINEAR_MODEL IMPORT RIDGE
RIDGEMODEL=RIDGE(ALPHA=0.1)
RIDGEMODEL.FIT(X,Y)
YHAT=RIDGEMODEL.PREDICT(X)
============================================================================================================================================================================================================
GRID SEARCH
-ALLOWD US TO SCAN THROUGH MULTIPLE FREE PARAMETERS WITH FEW LINES OF CODE
	-ALPHA IN RIDGE REGRESSION IS CALLED A HYPERPARAMETER
-AUTOMATICALLY ITERATE OVER THE HYPERPARAMETERS USING CROSS VALIDATION 
-USE VALIDATION DATA TO PICK THE BEST HYPERPARAMETER
FROM SKLEARN.LINEAR_MODEL IMPORT RIDGE
FROM SKLEARN.MODEL_SELECTION IMPORT GRIDSEARCHCV
PARAMETERS1=[{'ALPHA':[0.001, 0.1, 1, 10, 100, 1000, 10000, 100000, 1000000]}]
RR=RIDGE()
GRID1=GRIDSEARCHCV(RR, PARAMETERS1,CV=4)
GRID1.FIT(X_DATA[['HORSEPOWER','WEIGHT','ENGINE','MPG']],Y_DATA)
GRID1.BEST_ESTIMATOR_
SCORES= GRID1.CV_RESULTS_
SCORES= ['MEAN_TEST_SCORE']

FROM SKLEARN.LINEAR_MODEL IMPORT RIDGE
FROM SKLEARN.MODEL_SELECTION IMPORT GRIDSEARCHCV
PARAMETERS2=[{'ALPHA':[0.001,0.1,1,10,100],'NORMALIZE':[TRUE, FALSE]}]
RR=RIDGE()
GRID1=GRIDSEARCHCV(RR,PARAMETERS2,CV=4)
GRID1.FIT(X_DATA[['HORSEPOWER','WEIGHT','ENGINE','MPG']],Y_DATA)
GRID1.BEST_ESTIMATOR_
SCORES=GRID1.CV_RESULTS_

FOR PARAM,MEAN_VAL,MEAN_TEST INZIP(SCORES['PARAMS'],SCORES['MEAN_TEST_SCORE'],SCORES['MEAN_TRAIN_SCORE']);
	PRINT(PARAM, "R^2 ON TEST DATA",MEAN_VAL,"R^2 ON TRAIN DATA",MEAN_TEST)